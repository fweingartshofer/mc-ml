{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Plant Species Classification\n",
    "\n",
    "## Analyze the data using the same techniques as for the last assignment.\n",
    "Decide for yourself which and how to use the specific commands. Answer\n",
    "the following questions in the report and include figures supporting your\n",
    "answers:\n",
    "\n",
    "### Which classes exist? Are they (roughly) balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import utils\n",
    "\n",
    "plt.rc('font', size=16)\n",
    "\n",
    "df = pd.read_csv('iris.csv')\n",
    "utils.ratio(df, 'Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes: Iris-setosa, Iris-versicolor, Iris-virginica\n",
    "They are perfectly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which noteworthy trends of features and relations between features as well as features and Classes do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df, hue='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.title(\"Correlation matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.box(by='Name',figsize=(40, 15), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PetalLength and PetalWidth correlate well.\n",
    "PetalLength and SepalWidth correlate negatively. (see correlation matrix above)\n",
    "PetalLength and PetalWidth are well segmented and can be used to distinguish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you would need to distinguish the classes with those features, which features would you choose, any why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PetalLength and PetalWidth because they don't overlap significantly. (see boxplot above)\n",
    "SepalLength and SepalWidth are not ideal to distinguish between flowers, since these features tend to overlap more, than any other feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In order to classify the three different Iris plant species, set up your first\n",
    "ML toolchain including the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Feature Preprocessing (if necessary and applicable)\n",
    "#### Are there any outliers in the data which might need to be removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['PetalLength', 'PetalWidth', 'SepalLength', 'SepalWidth']]\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the boxplot and the describe info, we do have some outliers that we could remove. Since we only have 150 samples it is not a good idea to simply remove the outliers (probably never is). A better approach would be to fix them to the mean / median or clip the outliers to some max value.\n",
    "\n",
    "We decided to go for the second method and simply clip the values to the 99% and 1% quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Name']\n",
    "q_max = X.quantile(.99)\n",
    "q_min = X.quantile(.01)\n",
    "# Outlier Removal\n",
    "\n",
    "X.clip(lower=q_min, upper=q_max, axis=1, inplace=True)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Are there any missing values which need to be taken care of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you need to apply any feature preprocessing steps? (e.g Normalization, Feature Deletion/Reduction/Addition)\n",
    "\n",
    "We do not need to apply normalization nor feature deletion but some models perform better on normalized data.\n",
    "With 150 samples feature deletion does not really provide any performance benefits, but we decided to do it anyway with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = preprocessing.StandardScaler().fit(X, y)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=[\"PetalLength\", \"PetalWidth\", \"SepalLength\", \"SepalWidth\"])\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.plot.density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to add some features to see if we can gain any useful information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled[\"PetalLengthSquared\"] = X_scaled[\"PetalLength\"] * X_scaled[\"PetalLength\"]\n",
    "X_scaled[\"PetalWidthSquared\"] = X_scaled[\"PetalWidth\"] * X_scaled[\"PetalWidth\"]\n",
    "X_scaled[\"SepalLengthSquared\"] = X_scaled[\"SepalLength\"] * X_scaled[\"SepalLength\"]\n",
    "X_scaled[\"SepalWidthSquared\"] = X_scaled[\"SepalWidth\"] * X_scaled[\"SepalWidth\"]\n",
    "\n",
    "X_scaled[\"Name\"] = y\n",
    "sns.pairplot(X_scaled, hue=\"Name\")\n",
    "X_scaled.drop(columns=[\"Name\"], inplace=True)  # remove target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Are there any categorical features that need to be transformed so that it can be used for classification task?\n",
    "    * No since all our features are numerical, we do not have any categorical features, besides the target feature `Name`.\n",
    "* Do you think it makes sense to derive any more features from the given ones? Why/why not?\n",
    "    * It could make sense, depending on the data. It is possible to generate information that can help a model perform better. Since we do not have a lot of samples we can definitely try to derive new features.\n",
    "* Split up the dataset into a training and a separate held back test set in a clever way\n",
    "    * Why is such a train/test split important?\n",
    "        * A: So we can validate our model and check whether we just made a lookup table of our data. It's our last safety line and important to measure the performance of our model.\n",
    "    * Which train/test split percentage do you choose and why?\n",
    "        * A: we choose a 70/30 split since we do not have a lot of samples and want enough data to validate our model and 70 / 30 % of 150 are integers.\n",
    "    * Think about how can you make sure to include samples from all three classes in both datasets and why this is important.\n",
    "        * A: If a class has no samples in our training data, the model can at best make a wild guess if a sample of that class is passed to the model. We ensured that every class is represented by using `sklearn.model_selection.train_test_split` and supplying it with the `stratify` parameter.\n",
    "\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "In the lecture we learned that feture selection like PCA or LDA should only be applied to the training data and not the test data, so we need to split our data now, we used a 30/70 split where we use 70% of our data for training and 30% for validation.\n",
    "\n",
    "* Use an appropriate cross-validation setup for the training:\n",
    "    * `X_train` and `y_train` represents our training data and `X_train` and `y_train` our held back test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.30)  # 70/30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "* Train different classification models to distinguish between the three Iris Plant Species:\n",
    "    * Use the following models: k Nearest Neighbour, Decision Tree, Support Vector Machine\n",
    "\n",
    "\n",
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm, neighbors, neural_network\n",
    "\n",
    "knn = GridSearchCV(\n",
    "    estimator= neighbors.KNeighborsClassifier(),\n",
    "    param_grid= [{'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'leaf_size': [15, 20]}],\n",
    "    scoring= \"accuracy\",\n",
    "    cv= 3)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = GridSearchCV(\n",
    "    estimator= DecisionTreeClassifier(),\n",
    "    param_grid= [{\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [10, 100, 1000],\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'class_weight': ['balanced']}],\n",
    "        scoring= \"accuracy\",\n",
    "        cv= 3\n",
    "    )\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "tree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "svc = GridSearchCV(\n",
    "    estimator= svm.SVC(),\n",
    "    param_grid= [{\n",
    "        'C': loguniform(0.1, 1, 100, 1000).rvs(20),\n",
    "        'class_weight': ['balanced'],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': loguniform(0.000035, 0.000245).rvs(20)\n",
    "    }]\n",
    ")\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = GridSearchCV(\n",
    "    estimator=neural_network.MLPClassifier(max_iter=10000),\n",
    "    param_grid= [{\n",
    "        'hidden_layer_sizes': [6, 9, 12],\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }]\n",
    ")\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "nn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use different hyperparameter settings for each model and explain why and how you chose them\n",
    "    * We selected each hyperparameter by trying different combinations, and then using the best fitting hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Estimates\n",
    "* Estimate the modelsâ€™ performances on the held back test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the models with their hyperparameter settings with two different error/performance measures\n",
    "* Why did you choose the specific error/performance measures?\n",
    "    * We chose the build-in report feature of sklearn, since it includes different scoring algorithms and scores for each label\n",
    "* What do they tell you?\n",
    "    * It tells us how well a model performs on the held-back testset, for each label and overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which model performs best with which hyperparameter settings and why do you think it does that way?\n",
    "    * The KNN and SVM Classifiers perform the best, because each Label is mostly cleanly seperated from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain which model you would use in deployment and why\n",
    "    * We would use the knn model, since it's the simplest model with the best score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
