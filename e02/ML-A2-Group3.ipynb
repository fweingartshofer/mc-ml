{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Plant Species Classification\n",
    "\n",
    "## Analyze the data using the same techniques as for the last assignment.\n",
    "Decide for yourself which and how to use the specific commands. Answer\n",
    "the following questions in the report and include figures supporting your\n",
    "answers:\n",
    "\n",
    "### Which classes exist? Are they (roughly) balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                 samples  ratio\nName                           \nIris-setosa           50    1.0\nIris-versicolor       50    1.0\nIris-virginica        50    1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>samples</th>\n      <th>ratio</th>\n    </tr>\n    <tr>\n      <th>Name</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Iris-setosa</th>\n      <td>50</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Iris-versicolor</th>\n      <td>50</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Iris-virginica</th>\n      <td>50</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import utils\n",
    "\n",
    "plt.rc('font', size=16)\n",
    "\n",
    "df = pd.read_csv('iris.csv')\n",
    "utils.ratio(df, 'Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes: Iris-setosa, Iris-versicolor, Iris-virginica\n",
    "They are perfectly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which noteworthy trends of features and relations between features as well as features and Classes do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df, hue='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.title(\"Correlation matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.box(by='Name',figsize=(40, 15), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PetalLength and PetalWidth correlate well.\n",
    "PetalLength and SepalWidth correlate negatively. (see correlation matrix above)\n",
    "PetalLength and PetalWidth are well segmented and can be used to distinguish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you would need to distinguish the classes with those features, which features would you choose, any why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PetalLength and PetalWidth because they don't overlap significantly. (see boxplot above)\n",
    "SepalLength and SepalWidth are not ideal to distinguish between flowers, since these features tend to overlap more, than any other feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In order to classify the three different Iris plant species, set up your first\n",
    "ML toolchain including the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Feature Preprocessing (if necessary and applicable)\n",
    "#### Are there any outliers in the data which might need to be removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['PetalLength', 'PetalWidth', 'SepalLength', 'SepalWidth']]\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the boxplot and the describe info, we do have some outliers that we could remove. Since we only have 150 samples it is not a good idea to simply remove the outliers (probably never is). A better approach would be to fix them to the mean / median or clip the outliers to some max value.\n",
    "\n",
    "We decided to go for the second method and simply clip the values to the 99% and 1% quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Name']\n",
    "q_max = X.quantile(.99)\n",
    "q_min = X.quantile(.01)\n",
    "# Outlier Removal\n",
    "\n",
    "X.clip(lower=q_min, upper=q_max, axis=1, inplace=True)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Are there any missing values which need to be taken care of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you need to apply any feature preprocessing steps? (e.g Normalization, Feature Deletion/Reduction/Addition)\n",
    "\n",
    "We do not need to apply normalization nor feature deletion but some models perform better on normalized data.\n",
    "With 150 samples feature deletion does not really provide any performance benefits, but we decided to do it anyway with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = preprocessing.StandardScaler().fit(X, y)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=[\"PetalLength\", \"PetalWidth\", \"SepalLength\", \"SepalWidth\"])\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.plot.density()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to add some features to see if we can gain any useful information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled[\"PetalLengthSquared\"] = X_scaled[\"PetalLength\"] * X_scaled[\"PetalLength\"]\n",
    "X_scaled[\"PetalWidthSquared\"] = X_scaled[\"PetalWidth\"] * X_scaled[\"PetalWidth\"]\n",
    "X_scaled[\"SepalLengthSquared\"] = X_scaled[\"SepalLength\"] * X_scaled[\"SepalLength\"]\n",
    "X_scaled[\"SepalWidthSquared\"] = X_scaled[\"SepalWidth\"] * X_scaled[\"SepalWidth\"]\n",
    "\n",
    "X_scaled[\"Name\"] = y\n",
    "sns.pairplot(X_scaled, hue=\"Name\")\n",
    "X_scaled.drop(columns=[\"Name\"], inplace=True)  # remove target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Are there any categorical features that need to be transformed so that it can be used for classification task?\n",
    "    * No since all our features are numerical, we do not have any categorical features, besides the target feature `Name`.\n",
    "* Do you think it makes sense to derive any more features from the given ones? Why/why not?\n",
    "    * It could make sense, depending on the data. It is possible to generate information that can help a model perform better. Since we do not have a lot of samples we can definitely try to derive new features.\n",
    "* Split up the dataset into a training and a separate held back test set in a clever way\n",
    "    * Why is such a train/test split important?\n",
    "        * A: So we can validate our model and check whether we just made a lookup table of our data. It's our last safety line and important to measure the performance of our model.\n",
    "    * Which train/test split percentage do you choose and why?\n",
    "        * A: we choose a 70/30 split since we do not have a lot of samples and want enough data to validate our model and 70 / 30 % of 150 are integers.\n",
    "    * Think about how can you make sure to include samples from all three classes in both datasets and why this is important.\n",
    "        * A: If a class has no samples in our training data, the model can at best make a wild guess if a sample of that class is passed to the model. We ensured that every class is represented by using `sklearn.model_selection.train_test_split` and supplying it with the `stratify` parameter.\n",
    "\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "In the lecture we learned that feture selection like PCA or LDA should only be applied to the training data and not the test data, so we need to split our data now, we used a 30/70 split where we use 70% of our data for training and 30% for validation.\n",
    "\n",
    "* Use an appropriate cross-validation setup for the training:\n",
    "    * `X_train` and `y_train` represents our training data and `X_train` and `y_train` our held back test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.30)  # 70/30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "* Train different classification models to distinguish between the three Iris Plant Species:\n",
    "    * Use the following models: k Nearest Neighbour, Decision Tree, Support Vector Machine\n",
    "\n",
    "\n",
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm, neighbors, neural_network\n",
    "\n",
    "knn = GridSearchCV(\n",
    "    estimator= neighbors.KNeighborsClassifier(),\n",
    "    param_grid= [{'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'leaf_size': [15, 20]}],\n",
    "    scoring= \"accuracy\",\n",
    "    cv= 3)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = GridSearchCV(\n",
    "    estimator= DecisionTreeClassifier(),\n",
    "    param_grid= [{\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [10, 100, 1000],\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'class_weight': ['balanced']}],\n",
    "        scoring= \"accuracy\",\n",
    "        cv= 3\n",
    "    )\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "tree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "svc = GridSearchCV(\n",
    "    estimator= svm.SVC(),\n",
    "    param_grid= [{\n",
    "        'C': loguniform(0.1, 1, 100, 1000).rvs(20),\n",
    "        'class_weight': ['balanced'],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': loguniform(0.000035, 0.000245).rvs(20)\n",
    "    }]\n",
    ")\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = GridSearchCV(\n",
    "    estimator=neural_network.MLPClassifier(max_iter=10000),\n",
    "    param_grid= [{\n",
    "        'hidden_layer_sizes': [6, 9, 12],\n",
    "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }]\n",
    ")\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "nn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use different hyperparameter settings for each model and explain why and how you chose them\n",
    "    * We selected each hyperparameter by trying different combinations, and then using the best fitting hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Estimates\n",
    "* Estimate the models’ performances on the held back test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the models with their hyperparameter settings with two different error/performance measures\n",
    "* Why did you choose the specific error/performance measures?\n",
    "    * We chose the build-in report feature of sklearn, since it includes different scoring algorithms and scores for each label\n",
    "* What do they tell you?\n",
    "    * It tells us how well a model performs on the held-back testset, for each label and overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which model performs best with which hyperparameter settings and why do you think it does that way?\n",
    "    * The KNN and SVM Classifiers perform the best, because each Label is mostly cleanly seperated from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain which model you would use in deployment and why\n",
    "    * We would use the knn model, since it's the simplest model with the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Boston House Price Prediction\n",
    "\n",
    "## Analyze the data using the same techniques as for the last assignment.\n",
    "Decide for yourself which and how to use the specific commands. Answer\n",
    "the following questions in the report and include figures supporting your\n",
    "answers:\n",
    "\n",
    "### Which noteworthy trends of features and relations between features as well as features and regression target do you see?\n",
    "\n",
    "* CRIM:\n",
    "    * per capita crime rate by town\n",
    "* ZN:\n",
    "    * proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS:\n",
    "    * proportion of non-retail business acres per town\n",
    "* CHAS:\n",
    "    * Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX:\n",
    "    * nitric oxides concentration (parts per 10 million)\n",
    "* RM:\n",
    "    * average number of rooms per dwelling\n",
    "* AGE:\n",
    "    * proportion of owner-occupied units built prior to 1940\n",
    "* DIS:\n",
    "    * weighted distances to five Boston employment centres\n",
    "* RAD:\n",
    "    * index of accessibility to radial highways\n",
    "* TAX:\n",
    "    * full-value property-tax rate per 10,000\n",
    "* PTRATIO:\n",
    "    * pupil-teacher ratio by town\n",
    "* B:\n",
    "    * 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT:\n",
    "    * % lower status of the population\n",
    "* MEDV (TAGET):\n",
    "    * Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"housing.csv\", sep=\"\\s+\",\n",
    "                 names=[\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"],\n",
    "                 header=None)\n",
    "\n",
    "df.describe() # list some statistics for the features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.title(\"Correlation matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It makes sense, that the `MEDV` correlates negatively with the `CRIM`, since demand for houses in areas with high crime rate would be lower than in those with lower rates.\n",
    "It also makes sense, that the prices in low crime rate areas fluctuate more than in those with higher crime rate, since this relation does not factor in other factors such as location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "g.map_upper(sns.scatterplot)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We think the most important features for our goal of predicting `MEDV` would be `RM` and `LSTAT` as they have the highest correlation with our target, though we can still see , that there are many outliers.\n",
    "The relationship between `LSTAT` and `MEDV` seems more like a curve as opposed to a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(20, 10)\n",
    "sns.regplot(df, x=\"RM\", y=\"MEDV\", ax=ax1)\n",
    "sns.regplot(df, x=\"LSTAT\", y=\"MEDV\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We do not really understand how this pair is supposed to have a correlation of 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(df, x=\"TAX\", y=\"RAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "sns.boxplot(df)\n",
    "plt.title(\"Boxplot for outlier detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Distribution / Histograms\n",
    "By plotting the histograms and fitting a kde, we can see that features such as `RM` and `MEDV` appear to have a normal distribution, while features such as `LSTAT`, `BIS`, and `NOX` are skewed to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,len(df.columns),figsize=(30,10))\n",
    "i = 0\n",
    "for column in df.columns:\n",
    "    sns.histplot(df[column], kde=True, ax=ax[i], color=\"lightblue\")\n",
    "    i += 1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Which features would you choose to train the regression models, any why?\n",
    "\n",
    "We think the features that would make the most sense would be `LSTAT` and `RM` since they have the highest correlation with our target `MEDV` (>= 0.7).\n",
    "From the correlation matrix we can see that we should exclude either `TAX` or `RAD` since they have a really high correlation. The same goes for `DIS` and `AGE` as they have a high negative correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build up your ML toolchain for this regression problem similar to the one you did for the classification and again take care of the following points:\n",
    "* Data and Feature Preprocessing (if necessary and applicable)\n",
    "* Train/Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df[\"MEDV\"]\n",
    "df.drop(columns=[\"MEDV\"], inplace=True)\n",
    "X = df\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_max = X.quantile(.99)\n",
    "q_min = X.quantile(.01)\n",
    "\n",
    "X.clip(lower=q_min, upper=q_max, axis=1, inplace=True)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X, y)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "# X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Density plots before/after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "ax1.set_title(\"before scaling\")\n",
    "\n",
    "fig.set_size_inches(20, 10)\n",
    "X.plot.density(ax=ax1)\n",
    "\n",
    "ax2.set_title(\"after scaling\")\n",
    "X_scaled.plot.density(ax=ax2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature processing:\n",
    "\n",
    "We noticed that creating arbitrary features (the ones below) makes our model perform way worse. When we used the code below with a pca that used 15 components we got a score of around 0.5 with linear regression and negative scores with polynomial regression. We only achieved higher results with these features after increasing the number of components to 20, after which we scored around 0.79.\n",
    "\n",
    "So we decided to not add any features and instead reduce the number of components for the pca.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add arbitrary features: the square of each feature and 2 / feature\n",
    "# for column in df.columns:\n",
    "#     X_scaled[column + \"_q\"] = X_scaled[column] ** 2\n",
    "#     X_scaled[\"2_div_\" + column] = 2 / X_scaled[column]\n",
    "# X_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create train and test split with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.30)  # 70/30 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Reduction\n",
    "Use PCA to reduce features, consider only the 5 most important pca components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5) # if features are added, change n_components >= 20 to achieve similar performance\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training\n",
    "* Use the following Regression models with different hyperparemter\n",
    "settings (where applicable) and an appropriate cross-validation setup\n",
    "for your training:\n",
    "    * Linear Regression\n",
    "    * Polynomial Regression\n",
    "    * Logistic Regression\n",
    "* Estimate the models’ performances on the test set again with two\n",
    "different error/performance measurements\n",
    "* Explain which model you would use in deployment and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDRegressor\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "best_score = -float(\"inf\")\n",
    "train_x = pd.DataFrame(X_train)\n",
    "train_y = pd.DataFrame(y_train)\n",
    "scores = []\n",
    "for train_idx, test_idx in KFold(n_splits=15).split(X_train):\n",
    "    kx_train, kx_test = train_x.iloc[train_idx], train_x.iloc[test_idx]\n",
    "    ky_train, ky_test = train_y.iloc[train_idx], train_y.iloc[test_idx]\n",
    "    m = LinearRegression()\n",
    "    m.fit(kx_train, ky_train)\n",
    "    current_score = m.score(kx_test, ky_test)\n",
    "    scores.append(current_score)\n",
    "    if current_score > best_score:\n",
    "        best_score = current_score\n",
    "        linear_reg = m\n",
    "print(\"Linear Regression - Scores: \", scores)\n",
    "print(\"Linear Regression - Best Score: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "We used 2 different ways of doing polynomial regression, once using kFold with polynom degree 2, and once using GridSearchCV by creating a pipeline for passing the hyperparams to PolynomialFeatures. GridSearchCV produced a different score with the same hyperparams, since it used different data to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "kFold_poly_reg = LinearRegression()\n",
    "best_score = -float(\"inf\")\n",
    "train_x = pd.DataFrame(X_poly)\n",
    "train_x.describe()\n",
    "train_y = pd.DataFrame(y_train)\n",
    "scores = []\n",
    "for train_idx, test_idx in KFold(n_splits=10).split(X_train):\n",
    "    kx_train, kx_test = train_x.iloc[train_idx], train_x.iloc[test_idx]\n",
    "    ky_train, ky_test = train_y.iloc[train_idx], train_y.iloc[test_idx]\n",
    "    m = LinearRegression()\n",
    "    m.fit(kx_train, ky_train)\n",
    "    current_score = m.score(kx_test, ky_test)\n",
    "    scores.append(current_score)\n",
    "    if current_score > best_score:\n",
    "        best_score = current_score\n",
    "        kFold_poly_reg = m\n",
    "print(\"kFold Polynomial Regression - Scores: \", scores)\n",
    "print(\"kFold Polynomial Regression - Best Score: \", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the pipeline object\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {'poly__degree': [2, 3, 4, 5], 'reg__fit_intercept': [True, False]}\n",
    "\n",
    "# Create a grid search object\n",
    "gridSearch_poly_reg = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "\n",
    "gridSearch_poly_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"GridSearch Polynomial Regression - Best Hyperparameters: {}\".format(gridSearch_poly_reg.best_params_))\n",
    "print(\"GridSearch Polynomial Regression - Best Score: {}\".format(gridSearch_poly_reg.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We can not use Logistic regression for predicting the house prices, as Logistic regression is a type of statistical model that is used for classification tasks. It is a regression model because it uses a linear combination of input features to make predictions, but it is different from traditional linear regression because the output variable is binary (i.e., it can take only two values, such as 0 or 1) rather than continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Performance estimates\n",
    "We use the following error/performance metrics to estimate our models:\n",
    "* Mean absolute error (MAE):\n",
    "    * This is the average of the absolute differences between the predicted values and the true values. It is computed using the mean_absolute_error() function from sklearn.\n",
    "* Mean squared error (MSE):\n",
    "    * This is the average of the squared differences between the predicted values and the true values. It is computed using the mean_squared_error() function from sklearn.\n",
    "* Root mean squared error (RMSE):\n",
    "    * This is the square root of the mean squared error. It is computed using the sqrt() function from the NumPy library.\n",
    "* R\\^2 score (R\\^2):\n",
    "    * This is the coefficient of determination, which is a measure of how well the model fits the data. It is computed using the r2_score() function from sklearn.\n",
    "* Mean absolute percentage error (MAPE):\n",
    "    * This is the average of the absolute percentage differences between the predicted values and the true values. It is often used when the true values are very small or zero, because it is not affected by the scale of the values.\n",
    "* Median absolute error (MedAE):\n",
    "    * This is the median of the absolute differences between the predicted values and the true values. It is similar to the mean absolute error, but it is more robust to outliers.\n",
    "* Explained variance score (EVS):\n",
    "    * This is a measure of how much of the variance in the true values is explained by the predicted values. It is computed using the explained_variance_score() function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error, explained_variance_score\n",
    "\n",
    "def performance_metrics(model_name, y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    medae = median_absolute_error(y_test, y_pred)\n",
    "    evs = explained_variance_score(y_test, y_pred)\n",
    "    return model_name, mae, mse, rmse, r2, mape, medae, evs\n",
    "\n",
    "performance_metrics_column_names = [\"model\", \"mae\", \"mse\", \"rmse\", \"r2\", \"mape\", \"meade\", \"evs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=performance_metrics_column_names)\n",
    "metrics_df.loc[len(metrics_df.index)] = performance_metrics(\"Linear Regression\", y_test, linear_reg.predict(X_test))\n",
    "metrics_df.loc[len(metrics_df.index)] = performance_metrics(\"kFold Polynomial Regression\", y_test, kFold_poly_reg.predict(poly.fit_transform(X_test)))\n",
    "metrics_df.loc[len(metrics_df.index)] = performance_metrics(\"GridSearch Polynomial Regression\", y_test, gridSearch_poly_reg.predict(X_test))\n",
    "# mark the best metric/error for each column depending on whether higher or lower is better (higher for perf and lower for errors\n",
    "#(metrics_df.style.highlight_min(color=\"green\", axis=0, subset=[\"mae\", \"mse\", \"rmse\", \"mape\", \"meade\"])).highlight_max(color=\"green\", axis=0, subset=[\"r2\", \"evs\"]).export()\n",
    "\n",
    "# sadly, the styler only works in the notebook not in the export\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model selection\n",
    "\n",
    "As we can see from the performance estimates, the polynomial regression definitely outperforms the linear regression.\n",
    "\n",
    "So the obvious chose would be one of the two polynomial regression model.\n",
    "Though it should be noted, that data vastly outside our training data can`t be predicted and in that case may perform worse than the linear regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Plot y_test against y_pred\n",
    "\n",
    "For fun we wanted to plot a regresseion between y_test and the prediction, if prediction were perfect, the correlation would be 1 and we would have a diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3)\n",
    "fig.set_size_inches(20, 5)\n",
    "\n",
    "ax1.set_title(\"Linear Regression\")\n",
    "sns.regplot(x=y_test, y=pd.DataFrame(linear_reg.predict(X_test))[0], ax=ax1)\n",
    "ax2.set_title(\"kFold Poly Reg\")\n",
    "sns.regplot(x=y_test, y=pd.DataFrame(kFold_poly_reg.predict(poly.fit_transform(X_test)))[0], ax=ax2)\n",
    "ax3.set_title(\"GridSearch Poly Reg\")\n",
    "sns.regplot(x=y_test, y=pd.DataFrame(gridSearch_poly_reg.predict(X_test))[0], ax=ax3)\n",
    "\n",
    "ax1.set_xlabel(\"y_test\")\n",
    "ax1.set_ylabel(\"y_pred\")\n",
    "ax2.set_xlabel(\"y_test\")\n",
    "ax2.set_ylabel(\"y_pred\")\n",
    "ax3.set_xlabel(\"y_test\")\n",
    "ax3.set_ylabel(\"y_pred\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
