{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "\n",
    "Import data, turn tags into a list of strings and define the available pitches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['name', 'duration', 'artist_genres', 'artist_names', 'acousticness',\n       'loudness', 'energy', 'danceability', 'mode', 'instrumentalness',\n       ...\n       'B_90', 'B_91', 'B_92', 'B_93', 'B_94', 'B_95', 'B_96', 'B_97', 'B_98',\n       'B_99'],\n      dtype='object', length=1240)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "\n",
    "pitch_symbol = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "\n",
    "df = pd.read_csv(\"songs-with-preprocessed-tags.csv\")\n",
    "# ignore songs that have no tags\n",
    "df = df[df[\"tags\"].notna()]\n",
    "\n",
    "# turn tags from strings into a list of strings\n",
    "df[\"artist_genres\"] = df[\"artist_genres\"].apply(literal_eval)\n",
    "df[\"tags\"] = df[\"tags\"].apply(literal_eval)\n",
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.iloc[:20, :25]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tolower = lambda s: s.lower()\n",
    "flatmap = lambda list_of_lists: [item for l in list_of_lists for item in l]\n",
    "tags = pd.Series(flatmap(df[~df[\"tags\"].isna()][\"tags\"].values.tolist())).apply(tolower)\n",
    "ratios = [ratio for ratio in tags.value_counts(normalize=True).to_list()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Visualization\n",
    "\n",
    "## Spotify Features\n",
    "\n",
    "Here we plot the correlation matrix of the features we selected from the spotify api."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df.iloc[:, :16].corr(numeric_only=True), annot=True)\n",
    "plt.title(\"Correlation matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only plot the spotify features here, since we have way too many columns.\n",
    "This is because we decided to use the pitches of every note as a feature. The problem is, that the amount of pitches is proportional to the length\n",
    "of the song. To reduce the amount of features, we preprocessed the pitches per note down to 100 values per note. Though this still leaves us with around 1240 feature columns.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pitches\n",
    "\n",
    "Since we have over ~100 different tags, we decided to take a closer look at the following tags, to\n",
    " get some sense fo our features, we decided to take a closer look at 5 different tags.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_by_tags = {\n",
    "    \"rock\": df[df[\"tags\"].apply(lambda x: \"rock\" in x)],\n",
    "    \"pop\": df[df[\"pop\"].apply(lambda x: \"pop\" in x)],\n",
    "    \"indie\": df[df[\"tags\"].apply(lambda x: \"indie\" in x)],\n",
    "    \"hip-hop\": df[df[\"tags\"].apply(lambda x: \"hip-hop\" in x)],\n",
    "    \"electronic\": df[df[\"tags\"].apply(lambda x: \"electronic\" in x)],\n",
    "    \"dance\": df[df[\"tags\"].apply(lambda x: \"dance\" in x)],\n",
    "    \"classic rock\": df[df[\"tags\"].apply(lambda x: \"classic rock\" in x)],\n",
    "    \"alternative rock\": df[df[\"tags\"].apply(lambda x: \"alternative rock\" in x)],\n",
    "    \"alternative\": df[df[\"tags\"].apply(lambda x: \"alternative\" in x)],\n",
    "    \"80s\": df[df[\"tags\"].apply(lambda x: \"80s\" in x)],\n",
    "}\n",
    "\n",
    "rock_df = df_by_tags[\"rock\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rock\n",
    "\n",
    "Here we check if the correlation matrix of just the songs tagged with `rock` is different from the correlation matrix of all songs.\n",
    "But as we can see, it looks pretty much the same."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(rock_df.iloc[:, :17].corr(numeric_only=True), annot=True)\n",
    "plt.title(\"Correlation matrix of songs tagged 'Rock'\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This plot show the avg pitches of the songs tagged with `rock`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.set_xticks(range(0, 101, 10))\n",
    "\n",
    "for i, note in enumerate(pitch_symbol):\n",
    "    curr_df = rock_df[[*[note + \"_\" + str(i) for i in range(0, 100)]]].mean().transpose()\n",
    "    curr_df.plot(ax=ax)\n",
    "ax.legend(pitch_symbol, loc=\"center right\")\n",
    "ax.set_title(\"Avg pitches for songs tagged with 'rock'\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots per tag\n",
    "### Pitches\n",
    "\n",
    "In order to compare the information above with other tags, we decided to plot the pitches separately and use the tags as the legend.\n",
    "\n",
    "And as we can see from the averages, `edm` is very different from the other tags. The `metal` tag has a very strong presents in E and B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(pitch_symbol) // 2, 2, figsize=(25, 50))\n",
    "\n",
    "for i, note in enumerate(pitch_symbol):\n",
    "    cur_ax = ax[i % (len(pitch_symbol) // 2)][i // (len(pitch_symbol) // 2)]\n",
    "    cur_ax.set_title(note)\n",
    "    cur_ax.legend([*df_by_tags.keys()])\n",
    "    for key, val in df_by_tags.items():\n",
    "        curr_df = val[[*[note + \"_\" + str(i) for i in range(0, 100)]]].mean().transpose()\n",
    "        curr_df.plot(ax=cur_ax, legend=False)\n",
    "\n",
    "for i in ax:\n",
    "    for axis in i:\n",
    "        axis.legend([*df_by_tags.keys()], loc=\"center right\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spotify features\n",
    "\n",
    "Here we decided to check if we could maybe differentiate our selected tags by plotting the boxplot of the spotify features for our selected tags.\n",
    "But as we can see, most of them have a high spread and overlap, so we do not think that they are very helpful."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spotify_features = ['duration', 'liveness', 'valence', 'danceability', 'tempo', 'loudness', 'energy',\n",
    "                    'acousticness']\n",
    "\n",
    "fig, ax = plt.subplots(len(spotify_features) // 2, 2, figsize=(25, 50))\n",
    "\n",
    "spotify_feature_df = pd.concat([\n",
    "    df_by_tags[\"rock\"].assign(tag=\"rock\"),\n",
    "    df_by_tags[\"metal\"].assign(tag=\"metal\"),\n",
    "    df_by_tags[\"hip-hop\"].assign(tag=\"hip-hop\"),\n",
    "#    df_by_tags[\"jazz\"].assign(tag=\"jazz\"),\n",
    "#    df_by_tags[\"electro\"].assign(tag=\"electro\"),\n",
    "#    df_by_tags[\"indie\"].assign(tag=\"indie\"),\n",
    "    df_by_tags[\"edm\"].assign(tag=\"edm\"),\n",
    "#    df_by_tags[\"classic\"].assign(tag=\"classic\"),\n",
    "    df_by_tags[\"piano\"].assign(tag=\"piano\"),\n",
    "])\n",
    "\n",
    "for i, feature in enumerate(spotify_features):\n",
    "    cur_ax = ax[i % (len(spotify_features) // 2)][i // (len(spotify_features) // 2)]\n",
    "    cur_ax.set_title(feature)\n",
    "    spotify_feature_df[[\"tag\", feature]].boxplot(ax=cur_ax, by=\"tag\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Songs per Tag\n",
    "\n",
    "By counting the number of times a tag appears, we can see that we have an incredibly imbalanced dataset.\n",
    "We think that this will mean that our classifier will try to tag most things with pop as it is by far the tag that appears the most often.\n",
    "In the plot we already decided to filter out all tags that appear less than 250 times as they add no real information to the graph."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flatmap = lambda list_of_lists: [item for l in list_of_lists for item in l]\n",
    "tags = pd.Series(flatmap(df[\"tags\"].values.tolist()))\n",
    "counts = tags.value_counts()\n",
    "counts.info()\n",
    "print()\n",
    "print(\"Top  genres\")\n",
    "print(counts)\n",
    "counts.plot.barh(figsize=(10, 20))\n",
    "plt.title(\"Number of songs tagged with x\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Preprocessing\n",
    "\n",
    "Transform the labels via the `MultiLabelBinarizer` to a numerical representation, scale the input data and create a held-back test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# transform tags with MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform([*df['tags']])\n",
    "\n",
    "X = df.drop(columns=['tags', 'artist_names', 'name', \"artist_genres\"])\n",
    "\n",
    "# scale input\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# create held-back test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=234634754)  # 70/30 split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Models like the `DecisionTree` might perform better, or even run at all, if they are provided with teh `class_weights` of the targets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[{0: 0.5039488017429193, 1: 63.810344827586206},\n {0: 0.5487298606306217, 1: 5.630324543610548},\n {0: 0.5104358219933799, 1: 24.455947136563875},\n {0: 198.26785714285714, 1: 0.5012641083521445},\n {0: 0.5025801195002716, 1: 97.39473684210526},\n {0: 0.75, 1: 1.5}]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.utils import class_weight\n",
    "flat_labels = [label for sublist in df['tags'] for label in sublist]\n",
    "label_counts = Counter(flat_labels)\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(flat_labels), y=flat_labels)\n",
    "class_weights_dict = dict(zip(np.unique(flat_labels), class_weights))\n",
    "\n",
    "# Create a list of class weight dictionaries for each label\n",
    "class_weights_list = []\n",
    "for i in range(y.shape[1]):\n",
    "    label_column = y[:,i]\n",
    "    label_counts = Counter(label_column)\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(label_column), y=label_column)\n",
    "    class_weights_list.append(dict(zip(np.unique(label_column), class_weights)))\n",
    "class_weights_list\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Selection\n",
    "\n",
    "Some models can't effectively use 1241 features, so for them, we need to reduce the amount of features.\n",
    "\n",
    "Other models like MLP however can use all features, so we do not throw away the other features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=200)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_grid=[{\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['ball_tree', 'kd_tree'],\n",
    "        'leaf_size': [1, 3],\n",
    "        'p': [1, 2, 4],\n",
    "        'metric': ['manhattan', 'cosine', 'euclidean'],\n",
    "    }],\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "knn.fit(X_train_pca, y_train)\n",
    "knn.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the trained model to predict the tags for the new songs\n",
    "predicted_tags = knn.predict(X_test_pca)\n",
    "predicted_tags_inversed = mlb.inverse_transform(predicted_tags)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_tags_inversed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix\n",
    "\n",
    "def confusion_matrix(y_true, y_actual):\n",
    "    mats = multilabel_confusion_matrix(y_true=y_true, y_pred=y_actual)\n",
    "    fig, axs = plt.subplots(nrows=len(mats)//2, ncols=2, figsize=(10, 15))\n",
    "    fig.tight_layout(pad=3)\n",
    "    plot_labels = mlb.inverse_transform(np.array([[1, 1, 1, 1, 1, 1]]))[0]\n",
    "    for idx, mat in enumerate(mats):\n",
    "        axs[idx//2,idx % 2].set_title(plot_labels[idx])\n",
    "        axs[idx//2,idx % 2].xaxis.tick_top()\n",
    "        mat_df = pd.DataFrame(mat, index=[i for i in [\"Positive\", \"Negative\"]], columns=[i for i in [\"Positive\", \"Negative\"]])\n",
    "        sns.heatmap(mat_df, annot=True, ax=axs[idx//2,idx % 2])\n",
    "    plt.show()\n",
    "\n",
    "def print_performance_report(y_pred):\n",
    "    # Calculate the accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: {:.2f}\".format(acc))\n",
    "\n",
    "    # Calculate the precision\n",
    "    pre = precision_score(y_test, y_pred, average=\"samples\", zero_division=False)\n",
    "    print(\"Precision: {:.2f}\".format(pre))\n",
    "\n",
    "    # Calculate the recall\n",
    "    rec = recall_score(y_test, y_pred, average=\"samples\")\n",
    "    print(\"Recall: {:.2f}\".format(rec))\n",
    "\n",
    "    # Calculate the F1-score\n",
    "    f1 = f1_score(y_test, y_pred, average=\"samples\")\n",
    "    print(\"F1-score: {:.2f}\".format(f1))\n",
    "    confusion_matrix(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_knn = predicted_tags\n",
    "print(\"KNN\")\n",
    "print_performance_report(y_pred_knn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    param_grid=[{\n",
    "        'criterion': ['gini'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [1, 2, 8, 16, 48],\n",
    "        'min_samples_leaf': [1, 10, 30],\n",
    "        'min_weight_fraction_leaf': [0.0, 0.0001, 0.0001**10],\n",
    "        'max_features': [None, 'sqrt', 'log2'],\n",
    "        'max_leaf_nodes': [None, 10, 100, 1000],\n",
    "        'min_impurity_decrease': [0.0, 0.0001, 0.0001**10],\n",
    "        'class_weight': [class_weights_list], #TODO(multilabel dict)\n",
    "        'ccp_alpha': [0.0, 0.0001, 0.0001**10]\n",
    "    }],\n",
    "    n_jobs=-1\n",
    ")\n",
    "tree.fit(X_train_pca, y_train)\n",
    "tree.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the trained model to predict the tags for the new songs\n",
    "predicted_tags = tree.predict(X_test_pca)\n",
    "predicted_tags_inversed = mlb.inverse_transform(predicted_tags)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_tags_inversed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_dec_tree = predicted_tags\n",
    "print(\"Decision Tree\")\n",
    "print_performance_report(y_pred_dec_tree)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200, 300, 400],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [12, 24, 36, 48, 64],\n",
    "    'min_samples_split': [2, 8, 32],\n",
    "    #    'min_samples_leaf': [1, 2, 4, 12],\n",
    "    #    'max_leaf_nodes': [None, 2, 8, 256, 512],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'n_jobs': [-2],\n",
    "    'class_weight': [class_weights_list, None]\n",
    "}\n",
    "etc = GridSearchCV(ExtraTreesClassifier(random_state=42), param_grid)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/multiclass.html#\n",
    "etc.fit(X_train_pca, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the trained model to predict the tags for the new songs\n",
    "predicted_tags = etc.predict(X_test_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "etc.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlb.inverse_transform(predicted_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_extra_trees = predicted_tags\n",
    "print(\"ExtraTrees\")\n",
    "print_performance_report(y_pred_extra_trees)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rfc = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_random_forest = rfc.predict(X_test)\n",
    "print(\"RandomForest\")\n",
    "print_performance_report(y_pred_random_forest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(150,150,150), (100,100,100), (150,200,150)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'learning_rate_init': [0.01, 0.001, 0.0001],\n",
    "}\n",
    "mlp = GridSearchCV(MLPClassifier(max_iter=1000), param_grid, cv=5)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_mlp = mlp.predict(X_test)\n",
    "print(\"RandomForest\")\n",
    "print_performance_report(y_pred_mlp)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
