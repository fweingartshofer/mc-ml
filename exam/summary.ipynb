{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "## Types of Machine Learning\n",
    "### Supervised ML\n",
    "Each input has an explicit output, like a label or number.\n",
    "Its goal is to find relations between in- and output.\n",
    "\n",
    "The trained model can be used in an application.\n",
    "\n",
    "#### Classification\n",
    "Has labels as target (output), these are discrete values.\n",
    "Used to predict e.g.  genre of a song.\n",
    "\n",
    "#### Regression\n",
    "Output is a numeric continuous value, like stock prices.\n",
    "\n",
    "### Unsupervised ML\n",
    "No output, the goal is to find relations between samples, e.g. clustering.\n",
    "\n",
    "### Others\n",
    "- Reinforcement learning, use a reward function with an agent, actions in an environment (states) in a feedback-update-loop to continuously update the model.\n",
    "- Deep learning, deep neural networks, combines supervised, unsupervised and some other techniques\n",
    "\n",
    "## Glossary\n",
    "- Model: Relations in data that we model. In supervised learning: regression/classification model that is trained on our data.\n",
    "- Model type/class: The underlying algorithm that is used to create the model, like SVM or KNN.\n",
    "- Model parameters: Model parameters are what the models learn from data during training *on its own*\n",
    "- Hyperparameters: They are the parameters for the algorithm and influence\n",
    "    - how the model learns from the data\n",
    "    - the model's complexity. Hyperparameters can be tuned to change the models' behaviour.\n",
    "- Training: Fitting the model to the data.\n",
    "- Evaluation/Test: Check how the model performs on test data.\n",
    "- Features/Predictors/Dimensions: measurable property, usually the columns in a csv.\n",
    "- Sample: one data point or one row in a csv-file\n",
    "\n",
    "## Basic ML Workflow\n",
    "![Basic ML Workflow](img/ml_workflow.png)\n",
    "\n",
    "## Type III Error\n",
    "*Provide the right answer to the wrong question.*\n",
    "\n",
    "Often occurs when the dataset is not fully understood or some kind of pattern occurs in it, that does not occur naturally.\n",
    "For example, model learns that every second sample is of type `A`.\n",
    "\n",
    "Use common sense and intuition!\n",
    "\n",
    "[Some examples](https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml?pli=1)\n",
    "\n",
    "# Visualization\n",
    "Visualization helps to understand data.\n",
    "It shows patterns in datasets.\n",
    "\n",
    "## What to look at first\n",
    "1. Before you even visualize anything, the basics: Size of dataframe? Datatypes? Class distributions? Basic stats per features? Is there data missing? (NA, etc?)\n",
    "2. Distribution of individual features: consider visualizing\n",
    "3. Relations between features: consider visualizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                  columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", 'target'])\n",
    "df[\"target\"] = df[\"target\"].apply(lambda x: iris.target_names[int(x)])\n",
    "df.shape  # the dimensions of the dataframe/dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dtypes  # the datatype of each dimension"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()  # shows for each numerical feature the mean, std, max, quantiles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification: class distribution\n",
    "Balanced data sets make things easier, but in reality often unequally distributed\n",
    "\n",
    "## Regression: distribution of target variable\n",
    "Similar problem: distribution not equal or does not cover complete target range\n",
    "\n",
    "## Scatterplot\n",
    "- pairplot, xyplot, etc\n",
    "- shows relations between 2 feature\n",
    "- Classification: color, symbol can indicate class label\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "sn.scatterplot(df[[\"sepal_length\", \"sepal_width\", \"target\"]], x=\"sepal_length\", y=\"sepal_width\", hue=\"target\", ax=ax[0])\n",
    "sn.scatterplot(df[[\"petal_length\", \"petal_width\", \"target\"]], x=\"petal_length\", y=\"petal_width\", hue=\"target\", ax=ax[1])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scatterplot Matrix\n",
    "Can show correlation between features, but can be messy with high dimensional datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(4, 4, figsize=(20, 20))\n",
    "\n",
    "for i, col in enumerate(df.drop(\"target\", axis=1).columns):\n",
    "    for j, inner_col in enumerate(df.drop(\"target\", axis=1).columns):\n",
    "        if col == inner_col:\n",
    "            ax[i, j].text(0.5, 0.5, col, ha='center', va='center', size=30)\n",
    "            break\n",
    "        sn.scatterplot(df[[col, inner_col, \"target\"]], x=col, y=inner_col, hue=\"target\", ax=ax[i, j])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Densityplot\n",
    "- Density of distribution of single variable\n",
    "- Sometimes plots individual samples with scatter for better understanding of data\n",
    "- Kernel parameter: defines granularity of density estimate\n",
    "- Classification: the more feature densities of different classes overlap, the more similar is the feature\n",
    "- Differences in density: feature poss. captures some differences in classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "for i, col in enumerate(df.drop(\"target\", axis=1)):\n",
    "    sn.kdeplot(df, x=col, hue=\"target\", ax=ax[i // 2, i % 2])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Histogram\n",
    "Nr. of bins change how many bars are used in a histogram.\n",
    "See also: Binning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2)\n",
    "sn.histplot(df, x=\"sepal_length\", bins=4, ax=ax[0])\n",
    "sn.histplot(df, x=\"sepal_length\", bins=8, ax=ax[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean, Median, SD and MAD\n",
    "All can be used for feature derivation\n",
    "\n",
    "### Mean and Median\n",
    "Show the \"center\" of samples distribution.\n",
    "Median is considered more statistically robust(to outliers) than mean\n",
    "\n",
    "### SD and median absolut deviation (MAD)\n",
    "Shows the scatter of samples. Again MAD is more robust than SD.\n",
    "\n",
    "SD Formula:\n",
    "$$\n",
    "  SD=\\sqrt{\\frac{\\sum (x_{i}-\\mu)^2}{N}}\n",
    "$$\n",
    "\n",
    "MAD Formula:\n",
    "`X` is the series of samples.\n",
    "`m(X)` can be arithmetic mean, median or mode, but for median absolute deviation it is the median!\n",
    "\n",
    "$$\n",
    "  MAD=\\vert\\frac{1}{n}\\sum_{i=1}^n x_{i} - mean(X)\\vert\n",
    "$$\n",
    "\n",
    "**MEDIAN and MAD are not impacted by outliers!!!**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sn.displot(df, x=\"sepal_length\", kind=\"kde\", height=8)\n",
    "plt.axvline(x=df[\"sepal_length\"].mean(), color='black', label=\"mean\")\n",
    "plt.axvline(x=df[\"sepal_length\"].median(), color='red', label=\"median\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2)\n",
    "mean = df[\"sepal_length\"].mean()\n",
    "sd = df[\"sepal_length\"].std()\n",
    "median = df[\"sepal_length\"].median()\n",
    "mad = (df[\"sepal_length\"] - df[\"sepal_length\"].mean()).abs().mean()\n",
    "\n",
    "sn.kdeplot(df, x=\"sepal_length\", ax=ax[0])\n",
    "ax[0].axvline(x=mean, color='black', label=\"mean\")\n",
    "ax[0].axvline(x=mean + sd, color='green', label=\"sd\")\n",
    "ax[0].axvline(x=mean - sd, color='green')\n",
    "ax[0].legend(loc=0)\n",
    "\n",
    "sn.kdeplot(df, x=\"sepal_length\", ax=ax[1])\n",
    "ax[1].axvline(x=median, color='red', label=\"median\")\n",
    "ax[1].axvline(x=median + mad, color='blue', label=\"mad\")\n",
    "ax[1].axvline(x=median - mad, color='blue')\n",
    "ax[1].legend(loc=1)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Boxplot (=box-and-whisker-plot)\n",
    "Shows distribution of one variable as the variables quartiles.\n",
    "\n",
    "Quartiles != quantiles\n",
    "\n",
    "It's a form of quantiles using 3 separations that results into 4 parts.\n",
    "Each part holds ~25% of the data samples.\n",
    "- Q1: 25% below, 75% above\n",
    "- Q2: median, 50% below, 50% above\n",
    "- Q3: 75% below, 25% above\n",
    "\n",
    "There are also percentiles: if X is the 80% percentile -> 80% of samples below X\n",
    "\n",
    "The dots in a boxplot are the extrem outliers, the box contains 50% of the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2)\n",
    "df.boxplot(ax=ax[0])\n",
    "q = df[\"sepal_length\"].quantile([0.25, 0.5, 0.75])\n",
    "sn.kdeplot(df, x=\"sepal_length\", ax=ax[1])\n",
    "ax[1].axvline(x=q[0.25], color='blue', label=\"Q1\")\n",
    "ax[1].axvline(x=q[0.5], color='red', label=\"Q2\")\n",
    "ax[1].axvline(x=q[0.75], color='green', label=\"Q3\")\n",
    "ax[1].legend(loc=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Levelplot / Contourplot\n",
    "Shows 3D data as 2D + color/gradient.\n",
    "Works well in a discrete space (3D grid) -> can be used to show model quality over a 2D hyperparameter space.\n",
    "\n",
    "**Always use a legend!**\n",
    "\n",
    "Could not use seaborn to create a levelplot, but a heatmap is possible.\n",
    "See correlation plot.\n",
    "\n",
    "# Classification and Regression\n",
    "## KNN - Classification\n",
    "\n",
    "Select a class for a new sample based on the class of the K nearest neighbours.\n",
    "Therefore: KNN is non-linear, and K influences behavior\n",
    "\n",
    "Hyperparameters:\n",
    "- K\n",
    "- Distance measures, like euclidean or manhattan. Feature scaling matters: differently scaled -> different distances -> different results\n",
    "\n",
    "Can be used for instance and lazy learning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g = sn.jointplot(data=df, x=\"sepal_length\", y=\"sepal_width\", hue=\"target\")\n",
    "g.plot_joint(sn.kdeplot, color=\"r\", zorder=0, levels=6)\n",
    "g.plot_marginals(sn.rugplot, color=\"r\", height=-.15, clip_on=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def generate_scatterplot_from_model(model, title, axis):\n",
    "    cmap = {\n",
    "        \"setosa\": \"blue\",\n",
    "        \"versicolor\": \"orange\",\n",
    "        \"virginica\": \"green\",\n",
    "    }\n",
    "    data = [[length, width, model.predict(np.array([length, width]).reshape(1, -1))[0]]\n",
    "            for length in np.arange(df[\"sepal_length\"].min(), df[\"sepal_length\"].max(), 0.1)\n",
    "            for width in np.arange(df[\"sepal_width\"].min(), df[\"sepal_width\"].max(), 0.1)]\n",
    "    res = pd.DataFrame(data, columns=[\"sepal_length\", \"sepal_width\", \"target\"])\n",
    "    sn.scatterplot(data=res, x=\"sepal_length\", y=\"sepal_width\", hue=\"target\", ax=axis, hue_order=cmap.keys())\n",
    "    axis.set_title(title)\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "X = df[[\"sepal_length\", \"sepal_width\"]]\n",
    "y = df[\"target\"]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X.values, y.values)\n",
    "generate_scatterplot_from_model(knn, \"KNN: K=1\", ax[0])\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X.values, y.values)\n",
    "generate_scatterplot_from_model(knn, \"KNN: K=3\", ax[1])\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X.values, y.values)\n",
    "generate_scatterplot_from_model(knn, \"KNN: K=5\", ax[2])\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X.values, y.values)\n",
    "generate_scatterplot_from_model(knn, \"KNN: K=10\", ax[3])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN - Regression\n",
    "Use vector y of output values of the K nearest neighbors N to predict output.\n",
    "\n",
    "- Unweighted combination: use, e.g. mean or median to derive new output value\n",
    "- Weighted combination: use distance to new sample to weight output values during combination:\n",
    "  $ Y_S = \\frac{\\sum_{n} Y_n\\cdot d(N_n)}{\\sum_n d(N_n)} $\n",
    "  with `d` being the distance function of the nth sample.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def generate_lineplot_from_model(model, title, axis):\n",
    "    data = pd.DataFrame([[width, model.predict([[width]])[0]] for width in np.arange(df[\"petal_width\"].min(), df[\"petal_width\"].max(), 0.1)], columns=[\"petal_width\", \"petal_length\"])\n",
    "    sn.lineplot(data, x=\"petal_width\", y=\"petal_length\", ax=axis, marker=\"o\")\n",
    "    axis.set_title(title)\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "X = df[[\"petal_width\"]]\n",
    "y = df[\"petal_length\"]\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, weights=\"uniform\")\n",
    "knn.fit(X.values, y.values)\n",
    "generate_lineplot_from_model(knn, \"KNN: uniform\", ax[0])\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, weights=\"distance\")\n",
    "knn.fit(X.values, y.values)\n",
    "generate_lineplot_from_model(knn, \"KNN: distance\", ax[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instance Based Learning\n",
    "- Uses data directly instead of deriving an explicit model\n",
    "- No explicit training = high speed training\n",
    "- Easy adaptation for new data: no training required\n",
    "- No abstraction of original data: storage requirements huge\n",
    "- Model size grows with amount of data\n",
    "- Therefore: low speed classification\n",
    "\n",
    "## Lazy Learning\n",
    "- Lazy means to delay a task until the result is requested\n",
    "- Lazy Learning: does not learn until learned model is requested\n",
    "\n",
    "# Regression\n",
    "## Linear Regression (Linear Model)\n",
    "- for n dimensional data is a flat hyperplane in n-1 dimensions to approximate data (regression) or separate data (classification)\n",
    "- Cannot capture non-linear relations (curves)\n",
    "- Called \"linear model\" LM\n",
    "- Model parameters (2D example)\n",
    "    - $y = kx + d$: `k`, `d` are the parameters\n",
    "    - $ax + by + c = 0$, model parameters: `a`, `b`, `c`\n",
    "- No Hyperparameters\n",
    "    - The one parameter would be the polynomial degree which is a constant `1`.\n",
    "\n",
    "### How to find the optimal model?\n",
    "**These concepts apply to many models!**\n",
    "#### Closed Form\n",
    "\n",
    "- Solve using equations: done as matrix operation in ML software\n",
    "- Resulting equation parameters correspond to plane that minimizes error respectively to the error function. for example SSE\n",
    "- Possible for linear and polynomial regression â€“ not possible for many other, more complex problems\n",
    "- Is a one step solution, therefore faster\n",
    "- Closed-Form usually restricted\n",
    "\n",
    "#### Gradient Descent\n",
    "- Solves the problem in iterations\n",
    "- Multistep-process, usually slower\n",
    "- Can be applied more broadly\n",
    "- -> More details later\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g = sn.lmplot(data=df, x='petal_length', y='petal_width', line_kws={'color': 'g'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression Error Measures\n",
    "**These can be used outside of regression as well**\n",
    "\n",
    "- **BAD** Absolute error $ = \\sum\\vert E_n \\vert $, scales with the amount of samples\n",
    "- **Mean absolute error** $ = \\sum\\frac{\\vert E_n \\vert}{n} $\n",
    "- **Median absolute error** $ = median(\\vert E_n \\vert) $, more robust to outliers.\n",
    "Careful: both mean and median absolute error are sometimes abbreviated as MAE\n",
    "- Sum of squared errors SSE $ = \\sum E_n^2 $, weights outliers stronger. Used in linear regression software\n",
    "- Mean squared error MSE $ = \\sum\\frac{E_n^2}{n} $, normalizes the SSE\n",
    "- **Root mean squared error** RMSE $ = \\sqrt{\\sum\\frac{E_n^2}{n}} $, used frequently. Regression with MSE or RMSE is called Least Squares Regression.\n",
    "\n",
    "## Effect of Outliers on Regression\n",
    "Regression tries to minimize error of the error function by fitting the hyperplane to the data.\n",
    "With outliers the hyperplane might be moved towards outliers.\n",
    "It should be where the majority of data is.\n",
    "By changing the error function the model will react differently to outliers.\n",
    "Multidimensional data makes this problem hard to recognize.\n",
    "\n",
    "To find errors in the linear model plot predicted and observed values.\n",
    "If the predicted values show a curve the linear model might be the wrong model, as the output indicates that the data is non-linear.\n",
    "This will show tendencies of predictions.\n",
    "- Bias: visible as offset\n",
    "- Variance: visible as spread\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
